{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "import librosa  \n",
    "import numpy as np  \n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(audio_path):\n",
    "    # Loading the audio file\n",
    "    data, sample_rate = librosa.load(audio_path, res_type='kaiser_fast')\n",
    "    \n",
    "    # Extracting features from the audio data\n",
    "    zcr = np.mean(librosa.feature.zero_crossing_rate(y=data).T, axis=0)\n",
    "\n",
    "    chroma_stft = np.mean(librosa.feature.chroma_stft(y=data, sr=sample_rate).T, axis=0)\n",
    "\n",
    "    mfcc = np.mean(librosa.feature.mfcc(y=data, sr=sample_rate).T, axis=0)\n",
    "\n",
    "    rms = np.mean(librosa.feature.rms(y=data).T, axis=0)\n",
    "\n",
    "    mel_spectrogram = np.mean(librosa.feature.melspectrogram(y=data, sr=sample_rate).T, axis=0)\n",
    "\n",
    "    spectral_contrast = np.mean(librosa.feature.spectral_contrast(y=data, sr=sample_rate).T, axis=0)\n",
    "\n",
    "    tonnetz = np.mean(librosa.feature.tonnetz(y=data, sr=sample_rate).T, axis=0)\n",
    "\n",
    "    spectral_rolloff = np.mean(librosa.feature.spectral_rolloff(y=data, sr=sample_rate).T, axis=0)\n",
    "    \n",
    "    poly_features = np.mean(librosa.feature.poly_features(y=data, sr=sample_rate).T, axis=0)\n",
    "\n",
    "    # Horizontally stacking features\n",
    "    features = np.hstack([zcr, chroma_stft, mfcc, rms, mel_spectrogram, spectral_contrast, tonnetz, spectral_rolloff, poly_features])\n",
    "\n",
    "    # Reshape features\n",
    "    features = features.reshape(-1, 1)\n",
    "\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining features and labels\n",
    "X = []\n",
    "Y = []\n",
    "base_dir = os.getcwd()\n",
    "dataset_dir = os.path.join(base_dir, 'filtered_dataset')\n",
    "emotion_list = ['Angry', 'Disgusted', 'Fearful', 'Happy', 'Neutral', 'Sad', 'Surprised']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting features\n",
    "total_files = sum([len(files) for r, d, files in os.walk(dataset_dir)])\n",
    "pbar = tqdm(total=total_files)\n",
    "\n",
    "for emotion in emotion_list:\n",
    "    emotion_dir = os.path.join(dataset_dir, emotion)\n",
    "    for file in os.listdir(emotion_dir):\n",
    "        if file.endswith('.wav'):\n",
    "            audio_path = os.path.join(emotion_dir, file)\n",
    "            features = extract_features(audio_path)\n",
    "            X.append(features)\n",
    "            Y.append(emotion)\n",
    "            pbar.update()\n",
    "\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving Features and Labels\n",
    "np.save('saved_features/data.npy', X)\n",
    "np.save('saved_features/labels.npy', Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Importing saved features\n",
    "# X = np.load('saved_features/data.npy')\n",
    "# Y = np.load('saved_features/labels.npy')\n",
    "\n",
    "# Converting to numpy arrays\n",
    "X = np.array(X)\n",
    "Y = np.array(Y)\n",
    "\n",
    "# Label Encoding\n",
    "lr = LabelEncoder()\n",
    "Y_enc = lr.fit_transform(Y)\n",
    "\n",
    "# Splitting the data\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y_enc, test_size=0.3, random_state=48)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler on the training data and transform both the training and test data\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the encoded labels\n",
    "for index, label in enumerate(lr.classes_):\n",
    "    print(f'{index}: {label}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "model = Sequential([\n",
    "    Input(shape=(X_train.shape[1], 1)),\n",
    "    Conv1D(32, kernel_size=3, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Dropout(0.2),\n",
    "\n",
    "    Conv1D(64, kernel_size=3, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Dropout(0.2),\n",
    "\n",
    "    Flatten(),\n",
    "    \n",
    "    Dense(64, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.5),\n",
    "    Dense(7, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = 'saved_model/model.keras'\n",
    "# Compiling the model\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# Defining the early stopping callback\n",
    "cp_callback = ModelCheckpoint(filepath=checkpoint_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')  # Set to False to save the entire model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train_scaled, Y_train, validation_data=(X_test_scaled, Y_test), epochs=100, batch_size=32, callbacks=[cp_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the model\n",
    "model.evaluate(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the training history\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a figure with two subplots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(6, 4))\n",
    "\n",
    "# Plot training loss and validation loss\n",
    "ax1.plot(history.history['loss'], label='Training Loss')\n",
    "ax1.plot(history.history['val_loss'], label='Validation Loss')\n",
    "ax1.set_xlabel('Epochs')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Training and Validation Loss')\n",
    "ax1.legend()\n",
    "\n",
    "# Plot training accuracy and validation accuracy\n",
    "ax2.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "ax2.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "ax2.set_xlabel('Epochs')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.set_title('Training and Validation Accuracy')\n",
    "ax2.legend()\n",
    "\n",
    "# Adjust spacing between subplots\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the model\n",
    "model.save('saved_model/model.keras')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "site_build_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
